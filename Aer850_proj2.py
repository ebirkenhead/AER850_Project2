import osfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.models import Sequentialfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Denseimport matplotlib.pyplot as pltfrom tensorflow.keras.optimizers import Adamfrom keras.layers import BatchNormalization, Dropoutfrom keras.regularizers import l2from tensorflow.keras import optimizers#2.1 - Data Processing#define the input image shape, required to be (100,100, 3) #Establish the train and validation data directory (use relative paths). The data is#split into 3 folders - Train, Validation and Test which contain 1600, 800 and 192 test#images.#Perform data augmentation such as re-scaling, shear range and zoom range by using#packages such as Keras’ image preprocessing pipeline, or torchvision transforms for#the train data and validation data (only apply re-scaling for validation).#Create the train and validation generator using Keras’s built-in imagedatasetfromdirectory#function which takes in the data directory, image target size, batch size (32), and#class mode (categorical) or by using PyTorch’s Dataloaderdef load_data_paths():    train_path = '/Users/emilybirkenhead/Documents/4th Yr/AER850 - Intro t oMachine Learning/Project 2/Data/Train'    validation_path = '/Users/emilybirkenhead/Documents/4th Yr/AER850 - Intro t oMachine Learning/Project 2/Data/Validation'    test_path = '/Users/emilybirkenhead/Documents/4th Yr/AER850 - Intro t oMachine Learning/Project 2/Data/Test'    print("Train Path:", train_path)    print("Validation Path:", validation_path)    print("Test Path:", test_path)    return train_path, validation_path, test_pathdef create_data_generators(train_path, validation_path, input_shape, batch_size=32):    train_datagen = ImageDataGenerator(        rescale=1/255, shear_range=0.2, zoom_range=0.2    )    validation_datagen = ImageDataGenerator(rescale=1/255)    train_generator = train_datagen.flow_from_directory(        train_path, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical'    )    validation_generator = validation_datagen.flow_from_directory(        validation_path, target_size=input_shape[:2], batch_size=batch_size, class_mode='categorical'    )    return train_generator, validation_generator       #2.2 - Nerual Network Architecture Design#Convolutional layers which are accessed through the Conv2D package which filters#the images and extract key features. The three main components to explore within#this layer are: number of filters, kernel size, and stride. (The main focus can be on#filters and kernel size).#The next layer is the MaxPooling2D layer which follows the convolutional layers and#pools the extracted features and takes the highest value.#The Flatten layer is then utilized after all the convolutions and max pooling layers,#which is applied before the dense layers.#Lastly, the fully connected Dense and Dropout layers are utilized perform the final#predictions. (The final dense should only have 4 neurons, correlating to the label#classes)def create_model(input_shape, num_filters=(32, 64, 128), filter_size=(3, 3),                 pool_size=(2, 2), num_dense_units=4, activation='relu', optimizer='adam', loss='categorical_crossentropy'):        model = Sequential()        model.add(Conv2D(32, (3, 3), strides=(1,1), activation=activation, input_shape=input_shape))    model.add(Conv2D(64, (3, 3), strides=(1,1), activation=activation))    model.add(Conv2D(128, (3, 3), strides=(1,1), activation=activation))    model.add(Conv2D(128, (3, 3), strides=(1,1), activation=activation))        model.add(MaxPooling2D(pool_size=(2, 2)))    model.add(MaxPooling2D(pool_size=(2, 2)))        model.add(Flatten())            model.add(Dense(128, activation=activation))    model.add(Dropout(0.2))        model.add(Dense(32, activation=activation))    model.add(Dropout(0.2))        model.add(Dense(num_dense_units, activation="softmax"))    model.add(Dropout(0.2))        model.add(BatchNormalization())        model.compile(optimizer=Adam(learning_rate=0.0001), loss=loss, metrics=['accuracy'])    return model    #2.3 - Hyperparameter Analysis#Added into to 2.2 create_model function#2.4 - Model Evaluationdef main():    train_path, validation_path, test_path = load_data_paths()    input_shape = (100, 100, 3)    batch_size = 35    train_generator, validation_generator = create_data_generators(        train_path, validation_path, input_shape, batch_size    )    model = create_model(input_shape)           history = model.fit(        train_generator,        epochs=30,  # Change to the desired number of epochs        validation_data=validation_generator,        verbose=1    )    # Create a test data generator    test_datagen = ImageDataGenerator(        rescale=1/255,         )        test_generator = test_datagen.flow_from_directory(        test_path,        target_size=input_shape[:2],        batch_size=batch_size,        class_mode='categorical'    )    # Evaluate the model on the test set    test_loss, test_accuracy = model.evaluate(test_generator)    print(f'Test Accuracy: {test_accuracy}')    plt.figure(figsize=(12, 4))    # Plot training & validation accuracy values    plt.subplot(1, 2, 1)    plt.plot(history.history['accuracy'])    plt.plot(history.history['val_accuracy'])    plt.title('Model Accuracy')    plt.xlabel('Epoch')    plt.ylabel('Accuracy')    plt.legend(['Train', 'Validation'], loc='upper left')    # Plot training & validation loss values    plt.subplot(1, 2, 2)    plt.plot(history.history['loss'])    plt.plot(history.history['val_loss'])    plt.title('Model Loss')    plt.xlabel('Epoch')    plt.ylabel('Loss')    plt.legend(['Train', 'Validation'], loc='upper left')    plt.tight_layout()    plt.show()# Call the main functionmain()